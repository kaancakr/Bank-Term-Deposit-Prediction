{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Bank Marketing Term Deposit Prediction\n",
        "\n",
        "Rebuilt notebook covering data cleaning, preprocessing, feature selection, model comparison, hyperparameter tuning, evaluation, and export of a deployable pipeline for Streamlit.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import joblib\n",
        "from pathlib import Path\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (\n",
        "    RocCurveDisplay,\n",
        "    PrecisionRecallDisplay,\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    f1_score,\n",
        "    roc_auc_score,\n",
        ")\n",
        "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold, cross_val_score, train_test_split\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "DATA_PATH = Path('bank-additional.csv')\n",
        "ARTIFACT_DIR = Path('artifacts')\n",
        "ARTIFACT_DIR.mkdir(exist_ok=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load data\n",
        "\n",
        "df = pd.read_csv(DATA_PATH, sep=';')\n",
        "print('Shape:', df.shape)\n",
        "print('\\nPreview:')\n",
        "display(df.head())\n",
        "\n",
        "print('\\nTarget distribution:')\n",
        "print(df['y'].value_counts(normalize=True).rename('proportion'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic cleaning: treat 'unknown' as missing for categorical cols\n",
        "\n",
        "df_clean = df.copy()\n",
        "cat_cols = df_clean.select_dtypes(include=['object']).columns.tolist()\n",
        "cat_cols.remove('y')\n",
        "for col in cat_cols:\n",
        "    df_clean[col] = df_clean[col].replace('unknown', np.nan)\n",
        "\n",
        "print('Missing values after marking unknowns:')\n",
        "print(df_clean.isna().sum())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train/test split\n",
        "\n",
        "X = df_clean.drop(columns=['y'])\n",
        "y = df_clean['y'].map({'no': 0, 'yes': 1})\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
        ")\n",
        "\n",
        "print('Train size:', X_train.shape, 'Test size:', X_test.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preprocessing + feature selection\n",
        "\n",
        "num_cols = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "cat_cols = X_train.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "numeric_transformer = Pipeline(\n",
        "    steps=[\n",
        "        ('imputer', SimpleImputer(strategy='median')),\n",
        "        ('scaler', StandardScaler()),\n",
        "    ]\n",
        ")\n",
        "\n",
        "categorical_transformer = Pipeline(\n",
        "    steps=[\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('encoder', OneHotEncoder(handle_unknown='ignore')),\n",
        "    ]\n",
        ")\n",
        "\n",
        "preprocess = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, num_cols),\n",
        "        ('cat', categorical_transformer, cat_cols),\n",
        "    ]\n",
        ")\n",
        "\n",
        "selector = SelectKBest(mutual_info_classif, k=30)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare baseline models via ROC-AUC\n",
        "\n",
        "models = {\n",
        "    'log_reg': LogisticRegression(max_iter=1000, class_weight='balanced'),\n",
        "    'rf': RandomForestClassifier(random_state=RANDOM_STATE),\n",
        "    'gboost': GradientBoostingClassifier(random_state=RANDOM_STATE),\n",
        "    'mlp': MLPClassifier(hidden_layer_sizes=(50,), max_iter=500, random_state=RANDOM_STATE),\n",
        "}\n",
        "\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
        "\n",
        "results = {}\n",
        "for name, clf in models.items():\n",
        "    pipe = Pipeline([\n",
        "        ('preprocess', preprocess),\n",
        "        ('selector', selector),\n",
        "        ('clf', clf),\n",
        "    ])\n",
        "    scores = cross_val_score(pipe, X_train, y_train, cv=cv, scoring='roc_auc', n_jobs=-1)\n",
        "    results[name] = {'roc_auc_mean': scores.mean(), 'roc_auc_std': scores.std()}\n",
        "\n",
        "results_df = pd.DataFrame(results).T.sort_values('roc_auc_mean', ascending=False)\n",
        "print(results_df)\n",
        "best_name = results_df.index[0]\n",
        "print('\\nBest baseline model:', best_name)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparameter tuning for the best baseline\n",
        "\n",
        "base_clf = models[best_name]\n",
        "\n",
        "param_distributions = {}\n",
        "if best_name == 'log_reg':\n",
        "    param_distributions = {\n",
        "        'clf__C': np.logspace(-2, 2, 10),\n",
        "        'clf__penalty': ['l2'],\n",
        "        'clf__solver': ['lbfgs', 'liblinear'],\n",
        "    }\n",
        "elif best_name == 'rf':\n",
        "    param_distributions = {\n",
        "        'clf__n_estimators': [100, 200, 400],\n",
        "        'clf__max_depth': [None, 5, 10, 20],\n",
        "        'clf__max_features': ['auto', 'sqrt', 0.5],\n",
        "        'clf__min_samples_split': [2, 5, 10],\n",
        "    }\n",
        "elif best_name == 'gboost':\n",
        "    param_distributions = {\n",
        "        'clf__n_estimators': [100, 200, 400],\n",
        "        'clf__learning_rate': [0.01, 0.05, 0.1],\n",
        "        'clf__max_depth': [2, 3, 4],\n",
        "    }\n",
        "else:  # mlp\n",
        "    param_distributions = {\n",
        "        'clf__hidden_layer_sizes': [(50,), (100,), (50, 50)],\n",
        "        'clf__alpha': [1e-4, 1e-3, 1e-2],\n",
        "        'clf__learning_rate_init': [0.001, 0.01],\n",
        "    }\n",
        "\n",
        "search_pipe = Pipeline([\n",
        "    ('preprocess', preprocess),\n",
        "    ('selector', selector),\n",
        "    ('clf', base_clf),\n",
        "])\n",
        "\n",
        "search = RandomizedSearchCV(\n",
        "    search_pipe,\n",
        "    param_distributions=param_distributions,\n",
        "    n_iter=10,\n",
        "    scoring='roc_auc',\n",
        "    cv=cv,\n",
        "    n_jobs=-1,\n",
        "    random_state=RANDOM_STATE,\n",
        ")\n",
        "\n",
        "search.fit(X_train, y_train)\n",
        "print('Best params:', search.best_params_)\n",
        "print('Best CV ROC-AUC:', search.best_score_)\n",
        "\n",
        "best_model = search.best_estimator_\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluation on hold-out test set\n",
        "\n",
        "y_pred = best_model.predict(X_test)\n",
        "y_proba = best_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "roc_auc = roc_auc_score(y_test, y_proba)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "print('ROC-AUC:', roc_auc)\n",
        "print('F1:', f1)\n",
        "print('\\nClassification report:\\n', classification_report(y_test, y_pred))\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "RocCurveDisplay.from_predictions(y_test, y_proba, ax=axes[0])\n",
        "PrecisionRecallDisplay.from_predictions(y_test, y_proba, ax=axes[1])\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[2])\n",
        "axes[2].set_xlabel('Predicted')\n",
        "axes[2].set_ylabel('Actual')\n",
        "axes[2].set_title('Confusion Matrix')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature importance via permutation importance on test set\n",
        "\n",
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "perm = permutation_importance(best_model, X_test, y_test, n_repeats=10, random_state=RANDOM_STATE, scoring='roc_auc')\n",
        "\n",
        "# Map back to feature names after preprocessing\n",
        "feature_names = best_model.named_steps['preprocess'].get_feature_names_out()\n",
        "selected_mask = best_model.named_steps['selector'].get_support()\n",
        "selected_features = feature_names[selected_mask]\n",
        "\n",
        "importances = pd.DataFrame({\n",
        "    'feature': selected_features,\n",
        "    'importance': perm.importances_mean\n",
        "}).sort_values('importance', ascending=False).head(15)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.barplot(data=importances, y='feature', x='importance')\n",
        "plt.title('Top permutation importances')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Persist model and metrics for Streamlit\n",
        "\n",
        "MODEL_PATH = ARTIFACT_DIR / 'final_model.pkl'\n",
        "METRICS_PATH = ARTIFACT_DIR / 'metrics.json'\n",
        "\n",
        "joblib.dump(best_model, MODEL_PATH)\n",
        "metrics = {\n",
        "    'roc_auc': float(roc_auc),\n",
        "    'f1': float(f1),\n",
        "    'best_params': search.best_params_,\n",
        "    'best_model': best_name,\n",
        "}\n",
        "METRICS_PATH.write_text(json.dumps(metrics, indent=2))\n",
        "\n",
        "print('Saved model to', MODEL_PATH)\n",
        "print('Saved metrics to', METRICS_PATH)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
